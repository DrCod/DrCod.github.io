---
title: 'Introduction to Practical Machine Learning - Part II'
date: 2020-02-10
permalink: /posts/2020/02/blog-post-2/
---

Welcome to part II of the introduction to practical machine learning tutorial!

In this tutorial, we are going to learn the : 
* Implementation of a house price prediction model using linear regression
* Advanced: Implementation of gradient descent


_Find [part I here](https://drcod.github.io/posts/2020/02/blog-post-2/),if you have not read it._


### House Price Prediction


<br/><img src="/images/linear_regression.svg" /><br/>

In this section , we are going  to implement a linear regression model to predict the prices of houses given the size of the house.

Our toy data:
```python
    x =np.array([563,312,220,500,623,543]) # house sizes
    y = np.array([6244,3644,2004,5924,9244,6044]) # house prices
```
 We will import the _stats_ library from the _scipy_ package. This libray contains a pre-built linear regression model. We are going to make use of that model in the following example.

Also, we will need a library to help us plot and visualize how our model makes extrapolations when we feed it with new data. For this, we will import the _matplotlib_ library.

```python
    from scipy import stats
    import matplotlib.pyplot as plt 
```

 * Fitting the model

 Now that we have all the libraries available, we will fit the linear regression model from the _stats_ package  with our data. This fitting process involves training the model on our toy data to accurately map the house sizes to the house prices.

 ```python
    from scipy import stats
    import matplotlib.pyplot as plt    
    import numpy as np 


    # numpy has an array function that can convert a python list into what is called a numpy array which is the appropriate format for numpy operations
   
    x =np.array([563,312,220,500,623,543]) # our data with 6 examples. 
    y = np.array([6244,3644,2004,5924,9244,6044]) # corresponding outputs of our data

    #Fitting the model
    
    #Calling the stats.linregress(x,y) function will automatically learn what the slope, the intercept and 2 other parameters are, and return us these 4 values.These values are statistically significant in understanding the correlation between the dependent x variable and the independent y variable.

    # When we execute the code below,it will return us the equation of line of best fit for our data, x and y
    # We can then use this equation which is our machine learning regression model to make predictions on new data. 
    slope, intercept, r_value,p_value,std_err = stats.linregress(x,y) 
    # because this function returns us 4 values in a specific order, we will need  to define 4 variables(slope, intercept, r_value,p_value,std_err) to receive and hold them

 ```

* Visualization

```python
    import matplotlib.pyplot as plt 

    plt.plot(x,y,'ro',color='red') # creates our plot
    plt.title('House Price Prediction') # sets the title of our plot
    plt.xlabel('Size of house')  # labels our x-axis
    plt.ylabel('Price') # labels our y-axis

    plt.axis([0,700,0,10000]) # defines the x-axis in the range of 0 to 700 and the y-axis in the range of 0 to 10000

    # y_predicted = x*slope + intercept. This is our machine learning model
    y_predicted = x*slope + intercept
    plt.plot(x, y_predicted, 'b')#plots the input x, against y_predicted
    plt.savefig('regress.png') #this will save the plot as an image for us
    plt.show() # displays the plot for us to see
```

* Making Predictions

```python
    newX =898 # a new house with size of 898
    
    newY = newX*slope + intercept # model predicts its price 

    print(newY)
```

### Putting it all together

Full code for our  house price prediction 

```python
    from scipy import stats
    import matplotlib.pyplot as plt

    #Fitting the model
    
    #Calling the stats.linregress(x,y) function will automatically learn what the slope, the intercept and 2 other parameters are, and return us these 4 values.These values are statistically significant in understanding the correlation between the dependent x variable and the independent y variable.

    # When we execute the code below,it will return us the equation of line of best fit for our data, x and y
    # We can then use this equation, our machine learning regression model to make predictions on new data
    slope, intercept, r_value,p_value,std_err = stats.linregress(x,y) 
    # because this function returns us 4 values in a specific order, we will need  to define 4 variables(slope, intercept, r_value,p_value,std_err) to hold them


    # Visualizing 

    plt.plot(x,y,'ro',color='red') # creates our plot
    plt.title('House Price Prediction') # sets the title of our plot
    plt.xlabel('Size of house')  # labels our x-axis
    plt.ylabel('Price') # labels our y-axis

    plt.axis([0,700,0,10000]) # defines the x-axis in the range of 0 to 700 and the y-axis in the range of 0 to 10000

    # y_predicted = x*slope + intercept. This is our machine learning model
    y_predicted = x*slope + intercept
    plt.plot(x, y_predicted, 'b')#plots the input x, against y_predicted
    plt.savefig('regress.png') #this will save the plot as an image for us
    plt.show() # displays the plot for us to see


```
<br/><img src="regress.png" /><br/>


## Advanced: Gradient Descent Implementation

* _Cost Function_

The goal of the cost function is to tell us how accurate our model predictions are, in comparison with the real world data.

<br/><img src="/images/cost-function.svg"><br/>

_m_ : number of instances/examples of our data used in training

_x<sup>i</sup>_: _i<sup>th</sup>_ input of our data used in training

_y<sup>i</sup>_:  _i<sup>th</sup>_ output of our data used in training

In order for our cost function to make accurate predictions, we have to make the best choices of parameters that will  minimize the total error from the cost function.

It is the job of gradient descent to find these parameters. It runs iteratively, taking _steps_ proportional to the negative of the gradient of the cost function at that point. It uses the gradient , because the gradient tells us the direction of change of our total error of our cost function ,whenever there is a change in parameters. Using this feedback, gradient descent either increases or decreases the _steps_ that it will take in a direction towards a point of least error.

<br/><img src="images/big_lr.png"/><br/>

For some given number of iterations , say <var>n</var>, we need to simultaneously update _theta_(our model parameters) for all _n_ ,thus <var>j = 0, 1 ,..., n<var>

<br/><img src="images/gradient-descent-1.svg"/><br/>

Where,

<img src="/images/alpha.svg"/> : the size of the gradient descent _steps_

* **Now, show  me the code!** 

Now let us implement the gradient descent algorithm.

```python
# let's import a couple of libraries 

import numpy as np #we need this to handle our feature matrix and vector operations. We are using-as-to rename our imported numpy to np 

# numpy has an array function that can be accessed with the '.' operator, and be used to convert a python list([]) into what is called a numpy array(which is basically a list). Numpy operations can only be performed on numpy arrays.

# Our data with 6 examples - Inputs/Features 
x =np.array([563,312,220,500,623,543])

#corresponding labels of our data - Outputs/Targets
y = np.array([6244,3644,2004,5924,9244,6044])

alpha = 0.001


# We expect that a larger house should cost more than a smaller house. So the larger the house is, the higher the cost. Because there exists some kind of a linear correlation here, let's use the equation of a straight line as our model hypothesis.
def model(x):
    """Model

    it calculates the predictions from the data x.

    :param x: our input data
    """
    #let's initialize our slope and intercept to zeros
    slope = np.zeros(1,1) # a scalar 
    intercept = np.zeros(1,1) # a scalar
    predictions = x*slope + intercept # equation of a straight line aka line of best fit
    return predictions 

#let's now write the code for our cost function
def cost_function(x,y):
    """Cost function

    it calculates the loss/error from our model hypothesis .

    :param x: set of training data(or test data)
    :param y: training set outputs(real world correct outputs)
    """
    num_examples = x.shape[0] # this returns 6,the number of items in our data,i.e x

    #let's now compute the loss aka the difference between our model predictions and the correct outputs(y)
    delta = model(x) - y
    # using the cost function equation above, let's now calculate our total loss over all our data
    loss = (1 / 2 * num_examples) * (delta.T @ delta) #delta.T converts the shape of delta from 6rows x 1column to 1row x 6columns
    
    return loss[0][0] # returns the loss value(which is only one value) from the loss matrix

#let's write a function to direct the steps that the gradient descent function takes for every learning parameter it picks
def gradient_step(alpha):
    """Gradient Descent Steps

    This takes one step of gradient descent for the theta parameters
    :param alpha: parameter for our learning rate(the size of the step for gradient descent)

    """
    num_examples = 6

    predictions = model(x)#let's calculate the predictions of our hypothesis function over all 6 examples


    delta = predictions - y # calculates the error and stores it in the delta variable
 
    theta = theta - alpha * (1 / num_examples) * (delta.T @ x).T # the update step where the theta parameters are updated

    theta =theta # updates the old theta parameters with the new theta parameters calculated in the line above

# let's write a function for our gradient descent based on the equation shown above
def gradient_descent(alpha, iterations):
    """Gradient Descent

    it calculates what steps should be taken for each theta parameter in order to minimize the cost function.

    :param alpha: parameter for our learning rate(the size of the step for gradient descent)
    :param iterations: parameter for our gradient descent iterations

    """

    losses =[] # to hold the list of all previous errors from our cost function

    # for loop to run a specific number of times given by our iterations parameter
    for _ in range(iterations): 
        #The range(x) function returns a list of numbers from 0 to x-1. The '_' used here means that, we do not need the result we return from iterating through the values in the the list of numbers from 0 to iterations-1. 

        #perform a single gradient step
        gradient_step(alpha)

        loss = cost_function(x,y)
        losses.append(cost)

    return losses



# this will now train our model
def train(alpha, iterations=500):
    """Trains linear regression.
    :param alpha: learning rate (the size of the step for gradient descent)
    :param iterations: number of gradient descent iterations.
    """

    # Run gradient descent.
    cost_history = gradient_descent(alpha,iterations)

    return theta, cost_history

#run the code below
for i in range(15):
    print(i)
```

**Upcoming Tutorial: _Machine learning with Pandas and Numpy_**

 